# 第三章：計算機體系結構 (Computer Architecture)

## 中央處理器 CPU

> 圖 3.1。中央處理器 CPU

![](http://www.bottomupcs.com/chapter02/figures/computer.png)

> CPU 對寄存器中的值執行指令。這個示例首先將值R1設置為100，將值從內存位置0x100加載到R2中，將這兩個值相加並將結果放到R3中，最後將新值(110)存儲到R4(供進一步使用)。

<!--
> The CPU performs instructions on values held in registers. This example shows firstly setting the value of R1 to 100, loading the value from memory location 0x100 into R2, adding the two values together and placing the result in R3 and finally storing the new value (110) to R4 (for further use).
-->

為了大大簡化，計算機由一個連接到內存的中央處理器(CPU)組成。上圖說明了所有計算機操作背後的一般原理。

CPU執行從內存中讀取的指令。有兩類指令

1. 將值從內存加載到寄存器並將值從寄存器存儲到內存。
2. 對存儲在寄存器中的值進行操作的那些。例如，在兩個寄存器中對值進行加、減、乘或除，執行按位運算(和，或，xor等)或執行其他數學運算(平方根、sin、cos、tan等)。


在本例中，我們只是向存儲在內存中的值添加100，然後將這個新結果存儲回內存中。
<!--
To greatly simplify, a computer consists of a central processing unit (CPU) attached to memory. The figure above illustrates the general principle behind all computer operations.

The CPU executes instructions read from memory. There are two categories of instructions

1. Those that load values from memory into registers and store values from registers to memory.
2. Those that operate on values stored in registers. For example adding, subtracting multiplying or dividing the values in two registers, performing bitwise operations (and, or, xor, etc) or performing other mathematical operations (square root, sin, cos, tan, etc).

So in the example we are simply adding 100 to a value stored in memory, and storing this new result back into memory.
-->

### 分支 Branching

除了加載或存儲之外，CPU的另一個重要操作是分支。在內部，CPU保存下一條要在指令指針中執行的指令的記錄。通常，指令指針按順序遞增，指向下一個指令;分支指令通常會檢查特定寄存器是否為零或是否設置了標誌，如果設置了，則會修改指針到另一個地址。因此，下一個要執行的指令將來自程序的不同部分;這就是循環和決策語句的工作方式。


例如，如果(x==0)這樣的語句可以通過查找兩個寄存器的or來實現，一個寄存器包含x，另一個寄存器為0;如果結果為零，則比較為真(即x的所有位都為零)，並且應該獲取語句的主體，否則將分支超過主體代碼。
<!--
Apart from loading or storing, the other important operation of a CPU is branching. Internally, the CPU keeps a record of the next instruction to be executed in the instruction pointer. Usually, the instruction pointer is incremented to point to the next instruction sequentially; the branch instruction will usually check if a specific register is zero or if a flag is set and, if so, will modify the pointer to a different address. Thus the next instruction to execute will be from a different part of program; this is how loops and decision statements work.

For example, a statement like if (x==0) might be implemented by finding the or of two registers, one holding x and the other zero; if the result is zero the comparison is true (i.e. all bits of x were zero) and the body of the statement should be taken, otherwise branch past the body code.
-->
### 週期 Cycles

我們都熟悉計算機的速度，以兆赫或千兆赫(百萬或千兆赫每秒)表示。這被稱為時鐘速度，因為它是計算機內部時鐘脈衝的速度。


脈衝在處理器內部被用來保持內部同步。在每一個滴答或脈衝，可以啟動另一個操作;想想時鐘就像一個人在敲鼓，以保持槳同步。

<!--
We are all familiar with the speed of the computer, given in Megahertz or Gigahertz (millions or thousands of millions cycles per second). This is called the clock speed since it is the speed that an internal clock within the computer pulses.

The pulses are used within the processor to keep it internally synchronised. On each tick or pulse another operation can be started; think of the clock like the person beating the drum to keep the rower's oars in sync.
-->
### 獲取，解碼，執行，存儲 (Fetch, Decode, Execute, Store)

執行一條指令由一個特定的事件週期組成;獲取、解碼、執行和存儲。

例如，要執行CPU上面的add指令必須

1. 取回:將指令從內存放入處理器。
2. 解碼:內部解碼它必須做什麼(在本例中為add)。
3. 執行:從寄存器中提取值，實際上將它們相加
4. 存儲:將結果存儲到另一個寄存器中。您可能還會看到將指令退役的術語。
<!--
Executing a single instruction consists of a particular cycle of events; fetching, decoding, executing and storing.

For example, to do the add instruction above the CPU must

1. Fetch : get the instruction from memory into the processor.
2. Decode : internally decode what it has to do (in this case add).
3. Execute : take the values from the registers, actually add them together
4. Store : store the result back into another register. You might also see the term retiring the instruction.
-->
### 查看CPU內部 Looking inside a CPU

在內部，CPU有許多不同的子組件，它們分別執行上述每個步驟，通常它們都可以彼此獨立地發生。這類似於物理生產線，其中有許多工作站，每個步驟都有特定的任務要執行。一旦完成，它可以將結果傳遞到下一個站點，並接受一個新的輸入來進行工作。
<!--
Internally the CPU has many different sub components that perform each of the above steps, and generally they can all happen independently of each other. This is analogous to a physical production line, where there are many stations where each step has a particular task to perform. Once done it can pass the results to the next station and take a new input to work on.
-->
> 圖 3.2。在 CPU 內部 (Inside the CPU)

![](http://www.bottomupcs.com/chapter02/figures/block.png)

> CPU由許多不同的子組件組成，每個子組件執行一個專用任務。
> 
> The CPU is made up of many different sub-components, each doing a dedicated task.

圖3.2 “在CPU內部” 顯示了一個非常簡單的框圖，說明了現代CPU的一些主要部分。


你可以看到指令進來並被處理器解碼。CPU有兩種主要類型的寄存器，一種用於整數計算，另一種用於浮點計算。浮點是一種用二進制形式表示十進制位的數字的方法，在CPU中處理方式不同。MMX(多媒體擴展)和SSE(單指令多數據流)或Altivec寄存器類似於浮點寄存器。


寄存器文件是CPU內部寄存器的集合名。在這下面，我們有中央處理器的部分來做所有的工作。


我們說過，處理器要麼將一個值加載或存儲到寄存器中，要麼將一個寄存器存儲到內存中，要麼對寄存器中的值執行一些操作。


算術邏輯單元(ALU)是CPU操作的核心。它接受寄存器中的值，並執行CPU能夠執行的任意多種操作。所有現代處理器都有一些alu，因此每個alu都可以獨立工作。事實上，奔騰處理器既有快的也有慢的ALUs;速度快的操作比較小(所以你可以在CPU上安裝更多)，但是只能做最常見的操作，慢的ALUs可以做所有的操作，但是比較大。


地址生成單元(AGU)處理與緩存和主內存的對話，將值放入寄存器，以便ALU操作，並將值從寄存器中取出回主內存。


浮點寄存器有相同的概念，但是對它們的組件使用稍有不同的術語。


例如，要執行CPU上面的add指令必須


1. 取回:將指令從內存放入處理器。
2. 解碼:內部解碼它必須做什麼(在本例中為add)。
3. 執行:從寄存器中提取值，實際上將它們相加
4. 存儲:將結果存儲到另一個寄存器中。您可能還會看到將指令退役的術語。
<!--
Figure 3.2, “Inside the CPU” shows a very simple block diagram illustrating some of the main parts of a modern CPU.

You can see the instructions come in and are decoded by the processor. The CPU has two main types of registers, those for integer calculations and those for floating point calculations. Floating point is a way of representing numbers with a decimal place in binary form, and is handled differently within the CPU. MMX (multimedia extension) and SSE (Streaming Single Instruction Multiple Data) or Altivec registers are similar to floating point registers.

A register file is the collective name for the registers inside the CPU. Below that we have the parts of the CPU which really do all the work.

We said that processors are either loading or storing a value into a register or from a register into memory, or doing some operation on values in registers.

The Arithmetic Logic Unit (ALU) is the heart of the CPU operation. It takes values in registers and performs any of the multitude of operations the CPU is capable of. All modern processors have a number of ALUs so each can be working independently. In fact, processors such as the Pentium have both fast and slow ALUs; the fast ones are smaller (so you can fit more on the CPU) but can do only the most common operations, slow ALUs can do all operations but are bigger.

The Address Generation Unit (AGU) handles talking to cache and main memory to get values into the registers for the ALU to operate on and get values out of registers back into main memory.

Floating point registers have the same concepts, but use slightly different terminology for their components.
-->
### 流水線 Pipelining

正如我們在上面看到的，當ALU將寄存器加在一起時，AGU將值寫回內存，這是完全獨立的，所以CPU沒有理由不能同時做這兩件事。我們在系統中也有多個alu，每個alu都可以在不同的指令上工作。最後，CPU可以使用它的浮點邏輯執行一些浮點操作，而整數指令也在運行中。這個過程稱為流水線[5]，能夠實現這個過程的處理器稱為超標量體系結構。所有現代處理器都是超標量。


另一個類比可能是把管道想像成一個裝滿玻璃球的軟管，除了我們的玻璃球是CPU的指令。理想情況下，你要把彈珠放在一端，一個接一個(每個時鐘脈衝一個彈珠)，把管子裝滿。一旦滿了，對於每一個彈珠(指令)，你推入所有其他的將移動到下一個位置，一個將會掉出結束(結果)。


然而，分支指令會對這個模型造成破壞，因為它們可能會導致執行從另一個地方開始，也可能不會。如果是流水線操作，那麼您基本上必須猜測分支的走向，這樣您就知道將哪些指令引入流水線。如果CPU預測正確，一切都會好起來的!相反，如果處理器沒有正確預測，它就浪費了大量時間，必須清空管道並重新啟動。


這個過程通常被稱為管道沖洗，類似於必須停止並清空所有的玻璃球從軟管!
<!--
As we can see above, whilst the ALU is adding registers together is completely separate to the AGU writing values back to memory, so there is no reason why the CPU can not be doing both at once. We also have multiple ALUs in the system, each which can be working on separate instructions. Finally the CPU could be doing some floating point operations with its floating point logic whilst integer instructions are in flight too. This process is called pipelining[5], and a processor that can do this is referred to as a superscalar architecture. All modern processors are superscalar.

Another analogy might be to think of the pipeline like a hose that is being filled with marbles, except our marbles are instructions for the CPU. Ideally you will be putting your marbles in one end, one after the other (one per clock pulse), filling up the pipe. Once full, for each marble (instruction) you push in all the others will move to the next position and one will fall out the end (the result).

Branch instruction play havoc with this model however, since they may or may not cause execution to start from a different place. If you are pipelining, you will have to basically guess which way the branch will go, so you know which instructions to bring into the pipeline. If the CPU has predicted correctly, everything goes fine![6] Conversely, if the processor has predicted incorrectly it has wasted a lot of time and has to clear the pipeline and start again.

This process is usually referred to as a pipeline flush and is analogous to having to stop and empty out all your marbles from your hose!
-->

<!--
### 分支預測 Branch Prediction

管道齊平，預測取，預測不取，分支延遲槽
-->

<!--
pipeline flush, predict taken, predict not taken, branch delay slots
-->

### 重新排列 Reordering

<!-- This bit is crap 這是垃圾 -->

事實上，如果CPU是軟管，它可以自由地重新排列軟管中的彈珠，只要彈珠彈出來的順序和你放彈珠的順序是一樣的。我們稱之為程序順序，因為這是指令在計算機程序中給出的順序。

<!--
In fact, if the CPU is the hose, it is free to reorder the marbles within the hose, as long as they pop out the end in the same order you put them in. We call this program order since this is the order that instructions are given in the computer program.
-->
> 圖3.3。重新排序緩衝區的例子 (Reorder buffer example)

```
1: r3 = r1 * r2
2: r4 = r2 + r3
3: r7 = r5 * r6
4: r8 = r1 + r7
```

考慮一個指令流，如圖3.3所示，“重新排序緩衝區示例”指令2需要等待指令1完全完成才能開始。這意味著管道在等待計算值時必須停止。類似地，指令3和4依賴於r7。然而，指令2和指令3之間完全沒有依賴關係;這意味著它們在完全獨立的寄存器上運行。如果我們交換指令2和3，我們可以得到管道更好的排序，因為處理器可以做有用的工作，而不是等待管道完成以獲得前面指令的結果。


然而，在編寫非常低級的代碼時，一些指令可能需要一些關於操作如何排序的安全性。我們稱之為需求內存語義。如果您需要獲得語義，這意味著對於這個指令，您必須確保前面所有指令的結果已經完成。如果您需要發佈語義，那麼您的意思是在此之後的所有指令都必須看到當前的結果。另一個更嚴格的語義是內存屏障或內存圍欄，它要求操作在繼續之前被提交到內存中。


在某些體系結構上，處理器保證了這些語義，而在另一些體系結構上，必須顯式地指定它們。大多數程序員不需要直接擔心它們，儘管您可能會看到術語。
<!--
Consider an instruction stream such as that shown in Figure 3.3, “Reorder buffer example” Instruction 2 needs to wait for instruction 1 to complete fully before it can start. This means that the pipeline has to stall as it waits for the value to be calculated. Similarly instructions 3 and 4 have a dependency on r7. However, instructions 2 and 3 have no dependency on each other at all; this means they operate on completely separate registers. If we swap instructions 2 and 3 we can get a much better ordering for the pipeline since the processor can be doing useful work rather than waiting for the pipeline to complete to get the result of a previous instruction.

However, when writing very low level code some instructions may require some security about how operations are ordered. We call this requirement memory semantics. If you require acquire semantics this means that for this instruction you must ensure that the results of all previous instructions have been completed. If you require release semantics you are saying that all instructions after this one must see the current result. Another even stricter semantic is a memory barrier or memory fence which requires that operations have been committed to memory before continuing.

On some architectures these semantics are guaranteed for you by the processor, whilst on others you must specify them explicitly. Most programmers do not need to worry directly about them, although you may see the terms.
-->
### 複雜指令 vs 精簡指令 (CISC vs RISC)

將計算機體系結構劃分為複雜指令集計算機(CISC)和簡化指令集計算機(RISC)是一種常見的方法。

注意，在第一個示例中，我們顯式地將值加載到寄存器中，執行加法操作，並將保存在另一個寄存器中的結果值存儲回內存。這是用於計算的RISC方法的一個示例——僅對寄存器中的值執行操作，並顯式地將值加載和存儲到內存中。

CISC方法可能只是一條從內存獲取值的指令，在內部執行加法操作並將結果寫回。這意味著指令可能需要很多週期，但最終兩種方法都實現了相同的目標。

所有現代架構都被認為是RISC架構[7]。

原因有很多

* 雖然RISC使得彙編編程變得更加複雜，因為幾乎所有的程序員都使用高級語言，並且把生成彙編代碼的繁重工作留給編譯器，所以其他的優點超過了這個缺點。
* 因為RISC處理器中的指令更簡單，所以晶片內部有更多的寄存器空間。從內存層次結構中我們知道，寄存器是最快的內存類型，最終所有的指令都必須對寄存器中保存的值執行，所以在其他條件相同的情況下，更多的寄存器會導致更高的性能。
* 由於所有指令在同一時間執行，流水線是可能的。我們知道流水線操作需要不斷地將指令流輸入處理器，所以如果一些指令需要很長時間，而另一些則不需要，那麼流水線操作就會變得非常複雜，難以發揮作用。
<!--
A common way to divide computer architectures is into Complex Instruction Set Computer (CISC) and Reduced Instruction Set Computer (RISC).

Note in the first example, we have explicitly loaded values into registers, performed an addition and stored the result value held in another register back to memory. This is an example of a RISC approach to computing -- only performing operations on values in registers and explicitly loading and storing values to and from memory.

A CISC approach may be only a single instruction taking values from memory, performing the addition internally and writing the result back. This means the instruction may take many cycles, but ultimately both approaches achieve the same goal.

All modern architectures would be considered RISC architectures[7].

There are a number of reasons for this

* Whilst RISC makes assembly programming becomes more complex, since virtually all programmers use high level languages and leave the hard work of producing assembly code to the compiler, so the other advantages outweigh this disadvantage.
* Because the instructions in a RISC processor are much more simple, there is more space inside the chip for registers. As we know from the memory hierarchy, registers are the fastest type of memory and ultimately all instructions must be performed on values held in registers, so all other things being equal more registers leads to higher performance.
* Since all instructions execute in the same time, pipelining is possible. We know pipelining requires streams of instructions being constantly fed into the processor, so if some instructions take a very long time and others do not, the pipeline becomes far to complex to be effective.
-->
### EPIC 處理器

在本書的許多示例中都使用了Itanium處理器，它是一種被稱為《顯式並行指令計算》(Explicitly Parallel Instruction Computing, EPIC) 的經過修改的體系結構的示例。


我們已經討論了超級定標處理器 (superscaler processors) 是如何在不同的處理器部件中同時有許多指令的。顯然，為了使這一功能能夠正常工作，處理器應該按照能夠充分利用CPU可用元素的順序給出指令。


傳統上，組織傳入指令流是硬件的工作。程序按順序發出指令;處理器必須向前看，並試圖決定如何組織傳入的指令。


EPIC背後的理論是，在更高級別上有更多的可用信息，可以比處理器更好地做出這些決策。分析彙編語言指令流，就像當前的處理器一樣，會丟失許多程序員在原始原代碼中提供的信息。可以把它看作是研究莎士比亞戲劇和閲讀克利夫的筆記版本的區別。兩者都給了你相同的結果，但是原作有各種額外的信息來設置場景，讓你深入瞭解角色。


因此，排序指令的邏輯可以從處理器轉移到編譯器。這意味著編譯器編寫者需要更聰明地嘗試為處理器找到最佳的代碼排序。處理器也大大簡化了，因為它的很多工作都轉移到了編譯器。[8]

<!--
The Itanium processor, which is used in many example through this book, is an example of a modified architecture called Explicitly Parallel Instruction Computing.

We have discussed how superscaler processors have pipelines that have many instructions in flight at the same time in different parts of the processor. Obviously for this to work as well as possible instructions should be given the processor in an order that can make best use of the available elements of the CPU.

Traditionally organising the incoming instruction stream has been the job of the hardware. Instructions are issued by the program in a sequential manner; the processor must look ahead and try to make decisions about how to organise the incoming instructions.

The theory behind EPIC is that there is more information available at higher levels which can make these decisions better than the processor. Analysing a stream of assembly language instructions, as current processors do, loses a lot of information that the programmer may have provided in the original source code. Think of it as the difference between studying a Shakespeare play and reading the Cliff's Notes version of the same. Both give you the same result, but the original has all sorts of extra information that sets the scene and gives you insight into the characters.

Thus the logic of ordering instructions can be moved from the processor to the compiler. This means that compiler writers need to be smarter to try and find the best ordering of code for the processor. The processor is also significantly simplified, since a lot of its work has been moved to the compiler. [8]
-->
> [5] 實際上，任何現代處理器都有許多超過四個階段的管道，以上我們只展示了一個非常簡化的視圖。可以同時執行的階段越多，管道就越深。

> [6] 處理器(如奔騰處理器)使用跟蹤緩存來跟蹤分支的運行方式。在大多數情況下，它可以通過記住分支的前一個結果來預測分支的走向。例如，在發生100次的循環中，如果您記得分支的最後一個結果，那麼您將正確地執行99次，因為只有最後一次您才會繼續執行程序。

> [7] 甚至是最常見的架構，英特爾奔騰處理器，雖然有一個被分類為CISC的指令集，內部分解指令到晶片內部RISC風格的子指令，然後再執行。

> [8] EPIC中另一個經常使用的術語是非常長的指令世界(VLIW)，它是對處理器的每條指令進行擴展的地方，告訴處理器它應該在其內部單元的哪裡執行指令。這種方法的問題是代碼完全依賴於處理器的模型。公司總是對硬件進行修改，讓客戶每次都重新編譯他們的應用程序，並維護一系列不同的二進制文件是不切實際的。
> 
> EPIC通過添加一個抽象層，以通常的計算機科學方式解決了這個問題。與顯式地指定指令應該執行的處理器的確切部分不同，EPIC創建了一個簡化的視圖，其中包含一些核心單元，如內存、整數和浮點數。
<!--
> [5] In fact, any modern processor has many more than four stages it can pipeline, above we have only shown a very simplified view. The more stages that can be executed at the same time, the deeper the pipeline.

> [6] Processors such as the Pentium use a trace cache to keep a track of which way branches are going. Much of the time it can predict which way a branch will go by remembering its previous result. For example, in a loop that happens 100 times, if you remember the last result of the branch you will be right 99 times, since only the last time will you actually continue with the program.

> [7] Even the most common architecture, the Intel Pentium, whilst having an instruction set that is categorised as CISC, internally breaks down instructions to RISC style sub-instructions inside the chip before executing.

> [8] Another term often used around EPIC is Very Long Instruction World (VLIW), which is where each instruction to the processor is extended to tell the processor about where it should execute the instruction in its internal units. The problem with this approach is that code is then completely dependent on the model of processor is has been compiled for. Companies are always making revisions to hardware, and making customers recompile their application every single time, and maintain a range of different binaries was impractical.
> 
> EPIC solves this in the usual computer science manner by adding a layer of abstraction. Rather than explicitly specifying the exact part of the processor the instructions should execute on, EPIC creates a simplified view with a few core units like memory, integer and floating point.

-->
## 內存 Memory

### 內存層次結構 Memory Hierarchy

CPU只能直接從位於處理器晶片上的緩存內存中獲取指令和數據。緩存內存必須從主系統內存(隨機訪問內存或RAM)加載。但是，RAM只在電源打開時保留其內容，因此需要存儲在更持久的存儲中。

我們把這些內存層稱為內存層次結構
<!--
The CPU can only directly fetch instructions and data from cache memory, located directly on the processor chip. Cache memory must be loaded in from the main system memory (the Random Access Memory, or RAM). RAM however, only retains its contents when the power is on, so needs to be stored on more permanent storage.

We call these layers of memory the memory hierarchy
-->
> 表3.1。內存層次結構 (Memory Hierarchy)

速度 Speed | 內存 Memory | 說明 Description
------|--------|----------------------
最快 Fastest | 緩存	Cache | 緩存內存實際上是內嵌在CPU中的內存。緩存內存非常快，通常只需要一個週期就可以訪問，但是由於它直接嵌入到CPU中，所以它的大小是有限制的。實際上，緩存內存有幾個子級別(稱為L1、L2、L3)，它們的速度都略有提高。 Cache memory is memory actually embedded inside the CPU. Cache memory is very fast, typically taking only once cycle to access, but since it is embedded directly into the CPU there is a limit to how big it can be. In fact, there are several sub-levels of cache memory (termed L1, L2, L3) all with slightly increasing speeds.
 | RAM  | | 處理器的所有指令和存儲地址必須來自RAM。雖然RAM非常快，但是CPU訪問它仍然需要花費大量的時間(這被稱為延遲)。RAM存儲在單獨的、與主板相連的專用晶片中，這意味著它比緩存內存大得多。 All instructions and storage addresses for the processor must come from RAM. Although RAM is very fast, there is still some significant time taken for the CPU to access it (this is termed latency). RAM is stored in separate, dedicated chips attached to the motherboard, meaning it is much larger than cache memory.
最慢的 Slowest | 磁碟 Disk | 我們都熟悉軟盤或CDROM上的軟件，並將文件保存到硬盤上。我們還熟悉一個程序從硬盤上加載所需要的很長時間——具有諸如旋轉磁碟和移動磁頭等物理機制意味著磁碟是最慢的存儲形式。但它們也是迄今為止最大的存儲形式。 We are all familiar with software arriving on a floppy disk or CDROM, and saving our files to the hard disk. We are also familiar with the long time a program can take to load from the hard disk -- having physical mechanisms such as spinning disks and moving heads means disks are the slowest form of storage. But they are also by far the largest form of storage.

關於內存層次結構的重要一點是在速度和大小之間進行權衡——內存越快越小。當然，如果你能找到改變這個等式的方法，你最終會成為億萬富翁!


緩存之所以有效，是因為計算機代碼通常具有兩種形式的局部性

1. 空間局部性表明，塊內的數據可能被一起訪問。
2. 時間局部性表明，最近使用的數據可能很快會再次使用。

這意味著通過實現儘可能多的快速訪問內存(時間)存儲小塊相關信息(空間)可以獲得好處。
<!--
The important point to know about the memory hierarchy is the trade offs between speed and size — the faster the memory the smaller it is. Of course, if you can find a way to change this equation, you'll end up a billionaire!

The reason caches are effective is because computer code generally exhibits two forms of locality

1. Spatial locality suggests that data within blocks is likely to be accessed together.
2. Temporal locality suggests that data that was used recently will likely be used again shortly.

This means that benefits are gained by implementing as much quickly accessible memory (temporal) storing small blocks of relevant information (spatial) as practically possible.
-->
### 深入緩存 (Cache in depth)

緩存是CPU體系結構中最重要的元素之一。要編寫高效的代碼，開發人員需要瞭解其系統中的緩存是如何工作的。


緩存是速度較慢的主系統內存的非常快的副本。緩存比主存儲器小得多，因為它與寄存器和處理器邏輯一起包含在處理器晶片中。從計算的角度來看，這是最好的房地產，它的最大規模既有經濟上的限制，也有物理上的限制。隨著製造商找到越來越多的方法將越來越多的晶體管塞到晶片上，晶片的緩存尺寸會大大增加，但即使是最大的緩存也會有幾十兆位元組，而不是普通的gb內存或tb的硬盤。


緩存由鏡像主內存的小塊組成。這些塊的大小稱為行大小，通常是32或64位元組。當談到緩存時，通常談論的是行大小，或者緩存線，它指的是鏡像主內存的一個塊。緩存只能以緩存線路的倍數的大小加載和存儲內存。


緩存有自己的層次結構，通常稱為L1、L2和L3。L1緩存是最快最小的;L2更大更慢，L3更大。


L1緩存通常被進一步劃分為指令緩存和數據，這被稱為“哈佛架構”(Harvard Architecture)。分割緩存有助於減少管道瓶頸，因為早期的管道階段傾向於引用指令緩存，而後期階段傾向於引用數據緩存。除了減少對共享資源的爭用之外，為指令提供單獨的緩存還允許利用指令流特性的替代實現;它們是只讀的，因此不需要昂貴的片上特性(比如多移植)，也不需要處理子塊讀取，因為指令流通常使用更常規的大小訪問。
<!--
Cache is one of the most important elements of the CPU architecture. To write efficient code developers need to have an understanding of how the cache in their systems works.

The cache is a very fast copy of the slower main system memory. Cache is much smaller than main memories because it is included inside the processor chip alongside the registers and processor logic. This is prime real estate in computing terms, and there are both economic and physical limits to its maximum size. As manufacturers find more and more ways to cram more and more transistors onto a chip cache sizes grow considerably, but even the largest caches are tens of megabytes, rather than the gigabytes of main memory or terabytes of hard disk otherwise common.

The cache is made up of small chunks of mirrored main memory. The size of these chunks is called the line size, and is typically something like 32 or 64 bytes. When talking about cache, it is very common to talk about the line size, or a cache line, which refers to one chunk of mirrored main memory. The cache can only load and store memory in sizes a multiple of a cache line.

Caches have their own hierarchy, commonly termed L1, L2 and L3. L1 cache is the fastest and smallest; L2 is bigger and slower, and L3 more so.

L1 caches are generally further split into instruction caches and data, known as the "Harvard Architecture" after the relay based Harvard Mark-1 computer which introduced it. Split caches help to reduce pipeline bottlenecks as earlier pipeline stages tend to reference the instruction cache and later stages the data cache. Apart from reducing contention for a shared resource, providing separate caches for instructions also allows for alternate implementations which may take advantage of the nature of instruction streaming; they are read-only so do not need expensive on-chip features such as multi-porting, nor need to handle handle sub-block reads because the instruction stream generally uses more regular sized accesses.
-->
> 圖3.4。緩存結合性 (Cache Associativity)

![](http://www.bottomupcs.com/chapter02/figures/sets.png)

給定的高速緩存線可能在一個陰影條目中找到一個有效的家。

在正常操作過程中，處理器不斷地要求緩存檢查某個特定地址是否存儲在緩存中，因此緩存需要一些方法來快速查找它是否有有效的行。如果給定地址可以緩存緩存內的任何地方,每一個高速緩存線路需要搜索每一次指的是確定衝擊或小姐繼續搜索快這樣做是在緩存中並行硬件,但搜索每個條目通常是過於昂貴的實現一個合理的大小的緩存。因此，通過對特定地址的地址進行限制，可以簡化緩存。這是一種權衡;緩存明顯比系統內存小得多，所以一些地址必須別名。如果兩個彼此別名的地址在不斷更新，那麼它們就會在緩存線路上發生衝突。因此，我們可以將緩存分類為三種一般類型，如圖3.4所示，“緩存結合性”。


* 直接映射緩存將允許緩存線只存在於緩存中的單個條目中。這是在硬件中最容易實現的，但是如圖3.4所示，“緩存結合性”沒有可能避免混疊，因為兩個陰影地址必須共享相同的緩存線。
* 完全關聯緩存將允許緩存線存在於緩存的任何條目中。這避免了混疊的問題，因為任何條目都可以使用。但是在硬件中實現非常昂貴，因為必須同時查找每個可能的位置，以確定某個值是否在緩存中。
* Set Associative cache 是 direct 和 fully Associative cache 的混合體，它允許一個特定的 cache 值存在於cache 中某些行的子集中。緩存被劃分為均勻的稱為方法的單元，一個特定的地址可以以任何方式定位。因此，n-way set associative cache 允許 cache line 存在於 set size total blocks mod n-圖3.4“cache associ”展示了一個8-element, 4-way set associative cache樣本;在這種情況下，這兩個地址有四個可能的位置，這意味著只有一半的緩存必須在查找時進行搜索。方法越多，可能的位置就越多，混疊也就越少，總體性能就越好。


一旦緩存滿了，處理器就需要去掉一行，為新行騰出空間。有許多算法，處理器可以選擇哪一行退出;例如，最近最少使用的(LRU) 是一種算法，其中最舊的未使用的行被丟棄，以便為新行騰出空間。


當數據僅從緩存中讀取時，不需要確保與主內存的一致性。然而，當處理器開始寫緩存線路時，它需要決定如何更新底層主存。在處理器更新緩存時，通過寫緩存將更改直接寫入主系統內存。這是比較慢的，因為寫入主內存的過程，如我們所見，比較慢。或者，寫回緩存延遲寫入到RAM中，直到絶對必要時。明顯的優點是，當寫入緩存條目時，需要較少的主內存訪問。已寫入但未提交到內存的緩存線路稱為dirty。缺點是，當緩存條目被清除時，它可能需要兩次內存訪問(一次是寫入髒數據主內存，另一次是加載新數據)。


如果一個條目同時存在於更高級別和更低級別的緩存中，我們說更高級別的緩存包含在內。或者，如果有一行的高級緩存消除了有該行的低級緩存的可能性，我們說它是排它的。這個選擇將在“SMP系統中的緩存獨占性”一節中進一步討論。

<!--
A given cache line may find a valid home in one of the shaded entries.


During normal operation the processor is constantly asking the cache to check if a particular address is stored in the cache, so the cache needs some way to very quickly find if it has a valid line present or not. If a given address can be cached anywhere within the cache, every cache line needs to be searched every time a reference is made to determine a hit or a miss. To keep searching fast this is done in parallel in the cache hardware, but searching every entry is generally far too expensive to implement for a reasonable sized cache. Thus the cache can be made simpler by enforcing limits on where a particular address must live. This is a trade-off; the cache is obviously much, much smaller than the system memory, so some addresses must alias others. If two addresses which alias each other are being constantly updated they are said to fight over the cache line. Thus we can categorise caches into three general types, illustrated in Figure 3.4, “Cache Associativity”.

* Direct mapped caches will allow a cache line to exist only in a singe entry in the cache. This is the simplest to implement in hardware, but as illustrated in Figure 3.4, “Cache Associativity” there is no potential to avoid aliasing because the two shaded addresses must share the same cache line.
* Fully Associative caches will allow a cache line to exist in any entry of the cache. This avoids the problem with aliasing, since any entry is available for use. But it is very expensive to implement in hardware because every possible location must be looked up simultaneously to determine if a value is in the cache.
* Set Associative caches are a hybrid of direct and fully associative caches, and allow a particular cache value to exist in some subset of the lines within the cache. The cache is divided into even compartments called ways, and a particular address could be located in any way. Thus an n-way set associative cache will allow a cache line to exist in any entry of a set sized total blocks mod n — Figure 3.4, “Cache Associativity” shows a sample 8-element, 4-way set associative cache; in this case the two addresses have four possible locations, meaning only half the cache must be searched upon lookup. The more ways, the more possible locations and the less aliasing, leading to overall better performance.

Once the cache is full the processor needs to get rid of a line to make room for a new line. There are many algorithms by which the processor can choose which line to evict; for example least recently used (LRU) is an algorithm where the oldest unused line is discarded to make room for the new line.

When data is only read from the cache there is no need to ensure consistency with main memory. However, when the processor starts writing to cache lines it needs to make some decisions about how to update the underlying main memory. A write-through cache will write the changes directly into the main system memory as the processor updates the cache. This is slower since the process of writing to the main memory is, as we have seen, slower. Alternatively a write-back cache delays writing the changes to RAM until absolutely necessary. The obvious advantage is that less main memory access is required when cache entries are written. Cache lines that have been written but not committed to memory are referred to as dirty. The disadvantage is that when a cache entry is evicted, it may require two memory accesses (one to write dirty data main memory, and another to load the new data).

If an entry exists in both a higher-level and lower-level cache at the same time, we say the higher-level cache is inclusive. Alternatively, if the higher-level cache having a line removes the possibility of a lower level cache having that line, we say it is exclusive. This choice is discussed further in the section called “Cache exclusivity in SMP systems”.
-->
### 緩存定址 Cache Addressing

到目前為止，我們還沒有討論緩存如何決定給定地址是否駐留在緩存中。顯然，緩存必須保存當前駐留在緩存行的數據的目錄。緩存目錄和數據可能共存於處理器上，但也可能是獨立的——例如POWER5處理器，它有一個內核上的L3目錄，但是實際訪問數據需要遍歷L3匯流排來訪問內核外的內存。這樣的安排可以促進更快的命中/錯過處理，而無需將整個緩存放在核心上的其他成本。
<!--
So far we have not discussed how a cache decides if a given address resides in the cache or not. Clearly, caches must keep a directory of what data currently resides in the cache lines. The cache directory and data may co-located on the processor, but may also be separate — such as in the case of the POWER5 processor which has an on-core L3 directory, but actually accessing the data requires traversing the L3 bus to access off-core memory. An arrangement like this can facilitate quicker hit/miss processing without the other costs of keeping the entire cache on-core.
-->
> 圖3.5。緩存標籤 (Cache tags)

![](http://www.bottomupcs.com/chapter02/figures/tags.png)

> 需要同時檢查標籤，以保持低延遲時間;更多的標籤位(即更少的集合結合性)需要更複雜的硬件來實現這一點。另外，更多的集合結合性意味著更少的標記，但是處理器現在需要硬件來複用許多集合的輸出，這也會增加延遲。
<!--
> Tags need to be checked in parallel to keep latency times low; more tag bits (i.e. less set associativity) requires more complex hardware to achieve this. Alternatively more set associativity means less tags, but the processor now needs hardware to multiplex the output of the many sets, which can also add latency.
-->
為了快速確定地址是否位於緩存中，它被分為三部分;標籤，索引和偏移量。


偏移量取決於緩存的行大小。例如，32位元組的行大小將使用地址的最後5位(即25位)作為該行的偏移量。


索引是條目可能駐留的特定高速緩存線。例如，讓我們考慮一個包含256個條目的緩存。如果這是一個直接映射的緩存，我們知道數據可能只駐留在一個可能的行中，因此偏移量之後的8位(28)描述要檢查的行——在0到255之間。


現在，考慮相同的256個元素緩存，但分為兩種方式。這意味著有兩組128行代碼，給定的地址可能位於這兩組中的任何一組。因此，只需要7位作為索引來偏移到128個條目的方式。對於給定的緩存大小，隨著方法的增加，索引所需的比特數也會減少，因為每一種方法都變得越來越小。


緩存目錄仍然需要檢查存儲在緩存中的特定地址是否是它感興趣的地址。因此，地址的剩餘位是標記位，緩存目錄根據傳入的地址標記位進行檢查，以確定是否有緩存命中。這種關係如圖3.5“緩存標記”所示。


當有多種方法時，這種檢查必須在每種方法中並行進行，然後將其結果傳遞給多路復用器，多路復用器輸出最後的命中或失敗結果。如上所述，緩存的結合度越高，索引所需的比特就越少，而標記比特就越多——就像完全結合式緩存那樣，沒有比特被用作索引比特。標籤位的並行匹配是高速緩存設計中昂貴的組成部分，通常是限制行數(i)的因素。多大)緩存可能增長。
<!--
To quickly decide if an address lies within the cache it is separated into three parts; the tag and the index and the offset.

The offset bits depend on the line size of the cache. For example, a 32-byte line size would use the last 5-bits (i.e. 25) of the address as the offset into the line.

The index is the particular cache line that an entry may reside in. As an example, let us consider a cache with 256 entries. If this is a direct-mapped cache, we know the data may reside in only one possible line, so the next 8-bits (28) after the offset describe the line to check - between 0 and 255.

Now, consider the same 256 element cache, but divided into two ways. This means there are two groups of 128 lines, and the given address may reside in either of these groups. Consequently only 7-bits are required as an index to offset into the 128-entry ways. For a given cache size, as we increase the number of ways, we decrease the number of bits required as an index since each way gets smaller.

The cache directory still needs to check if the particular address stored in the cache is the one it is interested in. Thus the remaining bits of the address are the tag bits which the cache directory checks against the incoming address tag bits to determine if there is a cache hit or not. This relationship is illustrated in Figure 3.5, “Cache tags”.

When there are multiple ways, this check must happen in parallel within each way, which then passes its result into a multiplexor which outputs a final hit or miss result. As describe above, the more associative a cache is, the less bits are required for index and the more as tag bits — to the extreme of a fully-associative cache where no bits are used as index bits. The parallel matching of tags bits is the expensive component of cache design and generally the limiting factor on how many lines (i.e, how big) a cache may grow.
-->
## 外圍設備和匯流排 Peripherals and buses

外設是連接到您的計算機的許多外部設備之一。顯然，處理器必須有某種方式與外圍設備進行通信，以使它們有用。


處理器和外圍設備之間的通信通道稱為匯流排。
<!--
Peripherals are any of the many external devices that connect to your computer. Obviously, the processor must have some way of talking to the peripherals to make them useful.

The communication channel between the processor and the peripherals is called a bus.
-->
### 外圍匯流排概念 Peripheral Bus concepts

設備需要輸入和輸出都有用。與外圍設備進行有用的通信需要一些常見的概念。
<!--
A device requires both input and output to be useful. There are a number of common concepts required for useful communication with peripherals.
-->
### 中斷 Interrupts

中斷允許設備直接中斷處理器來標記一些信息。例如，當按下一個鍵時，會生成一個中斷，將鍵按事件傳遞給操作系統。每個設備都由操作系統和 BIOS 的某種組合分配一個中斷。


設備通常連接到一個可編程中斷控制器(PIC)，這是一個獨立的晶片，它是主板的一部分，緩衝並將中斷信息傳遞給主處理器。每個設備之間都有物理中斷線，這是系統提供的PIC中的一個。當設備想要中斷時，它將修改這條線路上的電壓。


PIC角色的一個非常寬泛的描述是，它接收這個中斷並將其轉換為一條消息，供主處理器使用。雖然具體的過程因架構而異，但一般的原則是操作系統已經配置了一個中斷描述符表，它將每個可能的中斷與接收到中斷時要跳轉到的代碼地址配對。如圖3.6“處理中斷的概述”所示。


編寫這個中斷處理程序是設備驅動程序作者與操作系統一起完成的工作。
<!--
An interrupt allows the device to literally interrupt the processor to flag some information. For example, when a key is pressed, an interrupt is generated to deliver the key-press event to the operating system. Each device is assigned an interrupt by some combination of the operating system and BIOS.

Devices are generally connected to an programmable interrupt controller (PIC), a separate chip that is part of the motherboard which buffers and communicates interrupt information to the main processor. Each device has a physical interrupt line between it an one of the PIC's provided by the system. When the device wants to interrupt, it will modify the voltage on this line.

A very broad description of the PIC's role is that it receives this interrupt and converts it to a message for consumption by the main processor. While the exact procedure varies by architecture, the general principle is that the operating system has configured an interrupt descriptor table which pairs each of the possible interrupts with a code address to jump to when the interrupt is received. This is illustrated in Figure 3.6, “Overview of handling an interrupt”.

Writing this interrupt handler is the job of the device driver author in conjunction with the operating system.
-->
> 圖3.6。處理中斷的概述 (Overview of handling an interrupt)

![](http://www.bottomupcs.com/chapter02/figures/interrupt.png)

> 處理中斷的一般概述。設備將中斷提升到中斷控制器，中斷控制器將信息傳遞到處理器。處理器查看由操作系統填寫的描述符表，以查找處理故障的代碼。
<!--
> A generic overview of handling an interrupt. The device raises the interrupt to the interrupt controller, which passes the information onto the processor. The processor looks at its descriptor table, filled out by the operating system, to find the code to handle the fault.
-->
大多數驅動程序將中斷的處理分為上下兩部分。下半部分將確認中斷、隊列操作以進行處理，並將處理器返回到它正在快速執行的操作。上半部分將在CPU空閒時運行，併進行更深入的處理。這是為了停止中斷占用整個CPU。
<!--
Most drivers will split up handling of interrupts into bottom and top halves. The bottom half will acknowledge the interrupt, queue actions for processing and return the processor to what it was doing quickly. The top half will then run later when the CPU is free and do the more intensive processing. This is to stop an interrupt hogging the entire CPU.
-->
### 保存狀態 Saving state

由於中斷可以在任何時候發生，所以在完成中斷處理後，返回到正在運行的操作是非常重要的。操作系統的工作通常是確保在進入中斷處理程序時保存任何狀態;例如，在從中斷處理程序返回時註冊和恢復它們。這樣，除了丟失一些時間之外，中斷對於當時正在運行的任何事情都是完全透明的。
<!--
Since an interrupt can happen at any time, it is important that you can return to the running operation when finished handling the interrupt. It is generally the job of the operating system to ensure that upon entry to the interrupt handler, it saves any state; i.e. registers, and restores them when returning from the interrupt handler. In this way, apart from some lost time, the interrupt is completely transparent to whatever happens to be running at the time.
-->
### 中斷 v 陷阱和異常 (Interrupts v traps and exceptions)

雖然中斷通常與來自物理設備的外部事件相關聯，但是相同的機制對於處理內部系統操作非常有用。例如，如果處理器檢測到訪問無效內存、嘗試按零分割或指令無效等情況，它可以在內部引發一個由操作系統處理的異常。它也是一種機制，用於捕獲操作系統中的系統調用，如“系統調用”一節所述，並實現虛擬內存，如第 6 章“虛擬內存”所述。雖然是在內部生成的，而不是從外部源生成的，但是非同步中斷運行的代碼的原則是相同的。
<!--
While an interrupt is generally associated with an external event from a physical device, the same mechanism is useful for handling internal system operations. For example, if the processor detects conditions such as an access to invalid memory, an attempt to divide-by-zero or an invalid instruction, it can internally raise an exception to be handled by the operating system. It is also the mechanism used to trap into the operating system for system calls, as discussed in the section called “System Calls” and to implement virtual memory, as discussed in Chapter 6, Virtual Memory. Although generated internally rather than from an external source, the principles of asynchronously interrupting the running code remains the same.
-->
### 中斷的類型 Types of interrupts

在線路電平上有兩種主要的信號中斷方式和邊緣觸發方式。


電平觸發中斷定義中斷線路的高電壓，以指示中斷正在等待。邊緣觸發中斷檢測匯流排上的轉換;當線路電壓從低到高。對於邊觸發中斷，PIC檢測到一個方波脈衝，因為信號和中斷已經被觸發。


當設備共享一個中斷行時，差異就明顯了。在水平觸發系統中，中斷線將會很高，直到所有引發中斷的設備都被處理並取消它們的中斷。


在邊緣觸發系統中，線路上的脈衝將向PIC顯示中断發生，並向操作系統發出信號進行處理。然而，如果進一步的脈衝從另一個設備進入已經斷言的行。


水平觸發中斷的問題是，它可能需要相當多的時間來處理設備的中斷。在此期間，中斷線保持高，並且無法確定是否有其他設備在線路上引發中斷。這意味著在服務中斷時可能存在相當大的不可預測的延遲。


對於邊觸發中斷，可以注意到一個長時間運行的中斷並將其排隊，但是其他共享線路的設備仍然可以在此過程中進行轉換(從而引發中斷)。然而，這帶來了新的問題;如果兩個設備同時中斷，可能會錯過其中一個中斷，或者環境或其他干擾可能會產生一個錯誤的中斷，應該忽略它。
<!--
There are two main ways of signalling interrupts on a line — level and edge triggered.

Level-triggered interrupts define voltage of the interrupt line being held high to indicate an interrupt is pending. Edge-triggered interrupts detect transitions on the bus; that is when the line voltage goes from low to high. With an edge-triggered interrupt, a square-wave pulse is detected by the PIC as signalling and interrupt has been raised.

The difference is pronounced when devices share an interrupt line. In a level-triggered system, the interrupt line will be high until all devices that have raised an interrupt have been processed and un-asserted their interrupt.

In an edge-triggered system, a pulse on the line will indicate to the PIC that an interrupt has occurred, which it will signal to the operating system for handling. However, if further pulses come in on the already asserted line from another device.

The issue with level-triggered interrupts is that it may require some considerable amount of time to handle an interrupt for a device. During this time, the interrupt line remains high and it is not possible to determine if any other device has raised an interrupt on the line. This means there can be considerable unpredictable latency in servicing interrupts.

With edge-triggered interrupts, a long-running interrupt can be noticed and queued, but other devices sharing the line can still transition (and hence raise interrupts) while this happens. However, this introduces new problems; if two devices interrupt at the same time it may be possible to miss one of the interrupts, or environmental or other interference may create a spurious interrupt which should be ignored.
-->
### 不可屏敝的中斷 Non-maskable interrupts

對於系統來說，能夠在特定時間屏蔽或防止中斷是很重要的。通常，可以暫停中斷，但是有一類特殊的中斷，稱為不可屏蔽中斷(NMI)，是這個規則的例外。典型的例子是重置中斷。


NMIs對於實現諸如系統監視器之類的東西很有用，在系統監視器中，NMI會定期升起，並設置一些必須得到操作系統承認的標誌。如果在下一個週期的NMI之前沒有看到確認，那麼可以認為系統沒有前進。另一種常用用法是分析系統。週期性的NMI可以被提出並用來評估處理器當前運行的代碼;隨著時間的推移，這將建立一個運行什麼代碼的概要，並創建一個非常有用的系統性能洞察。
<!--
It is important for the system to be able to mask or prevent interrupts at certain times. Generally, it is possible to put interrupts on hold, but a particular class of interrupts, called non-maskable interrupts (NMI), are the exception to this rule. The typical example is the reset interrupt.

NMIs can be useful for implementing things such as a system watchdog, where a NMI is raised periodically and sets some flag that must be acknowledged by the operating system. If the acknowledgement is not seen before the next periodic NMI, then system can be considered to be not making forward progress. Another common usage is for profiling a system. A periodic NMI can be raised and used to evaluate what code the processor is currently running; over time this builds a profile of what code is being run and create a very useful insight into system performance.
-->
### IO空間 (IO Space)

顯然，處理器需要與外圍設備通信，它通過IO操作來實現這一點。最常見的IO形式被稱為內存映射IO，其中設備上的寄存器被映射到內存中。


這意味著，要與設備通信，只需對內存中的特定地址進行讀寫即可。
<!--
Obviously the processor will need to communicate with the peripheral device, and it does this via IO operations. The most common form of IO is so called memory mapped IO where registers on the device are mapped into memory.

This means that to communicate with the device, you need simply read or write to a specific address in memory. 

待辦事項:擴大 TODO: expand -->

### 直接存儲器存取 DMA

由於設備的速度遠低於處理器的速度，需要有某種方法來避免CPU等待來自設備的數據。

直接內存訪問(DMA)是一種在外設和系統RAM之間直接傳輸數據的方法。

驅動程序可以設置一個設備來進行DMA傳輸，方法是將數據放入RAM的區域。然後它可以啟動DMA傳輸並允許CPU繼續執行其他任務。

一旦設備完成，它將發出一個中斷，並向驅動程序發出信號，傳輸完成。從這個時候起，設備的數據(比如磁碟的文件，或者視頻捕獲卡的幀)就在內存中，可以使用了。
<!--
Since the speed of devices is far below the speed of processors, there needs to be some way to avoid making the CPU wait around for data from devices.

Direct Memory Access (DMA) is a method of transferring data directly between an peripheral and system RAM.

The driver can setup a device to do a DMA transfer by giving it the area of RAM to put its data into. It can then start the DMA transfer and allow the CPU to continue with other tasks.

Once the device is finished, it will raise an interrupt and signal to the driver the transfer is complete. From this time the data from the device (say a file from a disk, or frames from a video capture card) is in memory and ready to be used.
-->
### 其他總線 (Other Buses)

其他匯流排連接PCI匯流排和外部設備。
<!--
Other buses connect between the PCI bus and external devices.
-->
### USB

從操作系統的角度來看，USB設備是一組端點組合成一個接口。端點可以是in或out，因此只能向一個方向傳輸數據。端點可以有許多不同的類型:


* 控制終端用於配置設備等。
* 中斷端點用於傳輸少量數據。他們比…更優先考慮…
* 批量端點，它傳輸大量數據，但沒有得到保證的時間限制。
* 同步傳輸是高優先級的實時傳輸，但如果他們錯過了，他們不會重試。這是為流數據，如視頻或音頻，沒有必要再發送數據。

可以有許多接口(由多個端點組成)，接口被分組到配置中。然而，大多數設備只有一個配置。
<!--
From an operating system point of view, a USB device is a group of end-points grouped together into an interface. An end-point can be either in or out and hence transfers data in one direction only. End-points can have a number of different types:

* Control end-points are for configuring the device, etc.
* Interrupt end-points are for transferring small amounts of data. They have higher priority than ...
* Bulk end-points, which transfer large amounts of data but do not get guaranteed time constraints.
* Isochronous transfers are high-priority real-time transfers, but if they are missed they are not re-tried. This is for streaming data like video or audio where there is no point sending data again.

There can be many interfaces (made of multiple end-points) and interfaces are grouped into configurations. However most devices only have a single configuration.

> 圖 3.7。UHCI控制器操作概述 (Overview of a UHCI controller operation)

![](http://www.bottomupcs.com/chapter02/images/uhci.png)

> 一個UCHI控制器的概述，來自英特爾文檔

> An overview of a UCHI controller, taken from Intel documentation.

圖3.7“UHCI控制器操作概述”顯示了通用主機控制器接口(UHCI)的概述。它提供了USB數據如何通過硬件和軟件的組合從系統中移動的概述。從本質上說，該軟件以指定格式設置數據模板，供主機控制器通過USB匯流排讀取和發送數據。


從概述的左上角開始，控制器有一個帶有計數器的幀寄存器，計數器每毫秒遞增一次。這個值用於索引到軟件創建的框架列表。該表中的每個條目指向一個傳輸描述符隊列。軟件在內存中設置這些數據，由主機控制器讀取，主機控制器是驅動USB匯流排的獨立晶片。軟件需要安排工作隊列，使90%的幀時間是同步數據，10%的剩餘中斷，控制和批量數據。


從圖中可以看到，數據連結的方式意味著同步數據的傳輸描述符只與一個特定的幀指針相關聯——換句話說，只有一個特定的時間段——之後就會被丟棄。然而，中斷，控制和批量數據都是排隊等時數據，因此，如果沒有在一個幀(時間段)傳輸，將在下一個幀(時間段)。


USB層通過USB請求塊或URBs進行通信。URB包含關於此請求與哪個端點相關的信息、數據、任何相關信息或屬性以及URB完成時要調用的回調函數。USB驅動程序以固定的格式向USB核心提交URBs, USB核心與上面的USB主機控制器協調管理它們。你的數據被USB核心發送到USB設備，當它完成時，你的回叫被觸發。
<!--
Figure 3.7, “Overview of a UHCI controller operation” shows an overview of a universal host controller interface, or UHCI. It provides an overview of how USB data is moved out of the system by a combination of hardware and software. Essentially, the software sets up a template of data in a specified format for the host controller to read and send across the USB bus.

Starting at the top-left of the overview, the controller has a frame register with a counter which is incremented periodically — every millisecond. This value is used to index into a frame list created by software. Each entry in this table points to a queue of transfer descriptors. Software sets up this data in memory, and it is read by the host controller which is a separate chip the drives the USB bus. Software needs to schedule the work queues so that 90% of a frame time is given to isochronous data, and 10% left for interrupt, control and bulk data..

As you can see from the diagram, the way the data is linked means that transfer descriptors for isochronous data are associated with only one particular frame pointer — in other words only one particular time period — and after that will be discarded. However, the interrupt, control and bulk data are all queued after the isochronous data and thus if not transmitted in one frame (time period) will be done in the next.

The USB layer communicates through USB request blocks, or URBs. A URB contains information about what end-point this request relates to, data, any related information or attributes and a call-back function to be called when the URB is complete. USB drivers submit URBs in a fixed format to the USB core, which manages them in co-ordination with the USB host controller as above. Your data gets sent off to the USB device by the USB core, and when its done your call-back is triggered.
-->
## 從小到大的系統 Small to big systems

正如摩爾定律所預測的那樣，計算能力一直在以迅猛的速度增長，而且絲毫沒有放緩的跡象。對於任何高端服務器來說，只包含一個CPU都是相對少見的。這是以多種不同的方式實現的。
<!--
As Moore's law has predicted, computing power has been growing at a furious pace and shows no signs of slowing down. It is relatively uncommon for any high end servers to contain only a single CPU. This is achieved in a number of different fashions.
-->
### 對稱多處理 Symmetric Multi-Processing

對稱多處理(通常簡稱為SMP)是目前在單個系統中包含多個cpu的最常見配置。


對稱術語指的是系統中的所有cpu都是相同的(例如架構、時鐘速度)。在SMP系統中，有多個處理器共享其他所有系統資源(內存、磁碟等)。
<!--
Symmetric Multi-Processing, commonly shortened to SMP, is currently the most common configuration for including multiple CPUs in a single system.

The symmetric term refers to the fact that all the CPUs in the system are the same (e.g. architecture, clock speed). In a SMP system there are multiple processors that share other all other system resources (memory, disk, etc).
-->
### 緩存一致性 Cache Coherency

在大多數情況下，系統中的cpu是獨立工作的;每個組件都有自己的一組寄存器、程序計數器等。儘管單獨運行，有一個組件需要嚴格的同步。


這是CPU緩存;記住，緩存是一個快速訪問內存的小區域，它鏡像存儲在主系統內存中的值。如果一個CPU修改了主內存中的數據，而另一個CPU的緩存中有該內存的舊副本，那麼系統顯然不會處於一致狀態。注意，只有當處理器寫入內存時才會出現這個問題，因為如果只讀取一個值，數據就會保持一致。


為了協調在所有處理器上保持緩存的一致性，SMP系統使用了窺探功能。窺探是處理器偵聽所有處理器連接到的匯流排上的緩存事件，並相應地更新其緩存。


其中一個協議是MOESI協議;代表修改，所有者，獨占，共享，無效。每一個都是一個高速緩存線可以在系統處理器上的狀態。還有其他協議可以做得更多，但是它們都有相似的概念。下面我們將介紹MOESI，以便您瞭解這個過程需要什麼。


當處理器需要從主存儲器中讀取高速緩存線路時，它首先必須檢查系統中所有其他的處理器，看看它們現在是否知道該內存區域的任何信息(例如緩存它)。如果它不存在於任何其他進程中，則處理器可以將內存加載到緩存中，並將其標記為獨占。當它寫到緩存時，它會改變要修改的狀態。在這裡，緩存的具體細節開始發揮作用;一些緩存會立即將修改後的緩存寫回系統內存(稱為寫通緩存，因為寫通過主內存)。其他的則不會這樣做，並且只將修改後的值保留在緩存中，直到它被清除，例如，當緩存變得滿時。


另一種情況是，處理器在另一個處理器緩存中探查並發現值。如果這個值已經被標記為已修改，它將把數據複製到自己的緩存中，並將其標記為共享。它將向另一個處理器(我們從它那裡得到數據)發送一條消息，將其緩存線標記為所有者。現在假設系統中的第三個處理器也想使用該內存。它將窺探並找到共享副本和所有者副本;因此，它將從所有者值中獲取其值。當所有其他處理器都只讀取這個值時，緩存線在系統中保持共享。然而，當一個處理器需要更新值時，它會通過系統發送一條失效消息。任何具有該高速緩存線的處理器都必須將其標記為無效，因為它不再反映“真實”值。當處理器發送invalidate消息時，它將緩存線標記為在其緩存中修改過的，而所有其他的都將標記為無效(請注意，如果緩存線是獨占的，則處理器知道沒有其他處理器依賴它，因此可以避免發送invalidate消息)。


從這一點開始，整個過程從頭開始。因此，無論哪個處理器具有修改後的值，都有責任在RAM從緩存中刪除時將真正的值寫回RAM。通過考慮協議，您可以看到這確保了處理器之間高速緩存線路的一致性。


隨著處理器數量的增加，這個系統有幾個問題。在只有幾個處理器的情況下，檢查另一個處理器是否具有緩存線(讀snoop)或使每個其他處理器中的數據無效(invalidate snoop)的開銷是可管理的;但是隨著處理器數量的增加，公交流量也在增加。這就是SMP系統通常只擴展到8個處理器的原因。


將所有處理器都放在同一匯流排上也開始出現物理問題。金屬線的物理特性只允許它們在一定的距離內排列，並且只有一定的長度。隨著處理器以千兆赫的速度運行，光速開始成為信息在系統中移動所需時間的真正考慮因素。


請注意，系統軟件通常不參與這個過程，儘管程序員應該知道硬件在做什麼，以響應他們為最大化性能而設計的程序。
<!--
For the most part, the CPUs in the system work independently; each has its own set of registers, program counter, etc. Despite running separately, there is one component that requires strict synchronisation.

This is the CPU cache; remember the cache is a small area of quickly accessible memory that mirrors values stored in main system memory. If one CPU modifies data in main memory and another CPU has an old copy of that memory in its cache the system will obviously not be in a consistent state. Note that the problem only occurs when processors are writing to memory, since if a value is only read the data will be consistent.

To co-ordinate keeping the cache coherent on all processors an SMP system uses snooping. Snooping is where a processor listens on a bus which all processors are connected to for cache events, and updates its cache accordingly.

One protocol for doing this is the MOESI protocol; standing for Modified, Owner, Exclusive, Shared, Invalid. Each of these is a state that a cache line can be in on a processor in the system. There are other protocols for doing as much, however they all share similar concepts. Below we examine MOESI so you have an idea of what the process entails.

When a processor requires reading a cache line from main memory, it firstly has to snoop all other processors in the system to see if they currently know anything about that area of memory (e.g. have it cached). If it does not exist in any other process, then the processor can load the memory into cache and mark it as exclusive. When it writes to the cache, it then changes state to be modified. Here the specific details of the cache come into play; some caches will immediately write back the modified cache to system memory (known as a write-through cache, because writes go through to main memory). Others will not, and leave the modified value only in the cache until it is evicted, when the cache becomes full for example.

The other case is where the processor snoops and finds that the value is in another processors cache. If this value has already been marked as modified, it will copy the data into its own cache and mark it as shared. It will send a message for the other processor (that we got the data from) to mark its cache line as owner. Now imagine that a third processor in the system wants to use that memory too. It will snoop and find both a shared and a owner copy; it will thus take its value from the owner value. While all the other processors are only reading the value, the cache line stays shared in the system. However, when one processor needs to update the value it sends an invalidate message through the system. Any processors with that cache line must then mark it as invalid, because it not longer reflects the "true" value. When the processor sends the invalidate message, it marks the cache line as modified in its cache and all others will mark as invalid (note that if the cache line is exclusive the processor knows that no other processor is depending on it so can avoid sending an invalidate message).

From this point the process starts all over. Thus whichever processor has the modified value has the responsibility of writing the true value back to RAM when it is evicted from the cache. By thinking through the protocol you can see that this ensures consistency of cache lines between processors.

There are several issues with this system as the number of processors starts to increase. With only a few processors, the overhead of checking if another processor has the cache line (a read snoop) or invalidating the data in every other processor (invalidate snoop) is manageable; but as the number of processors increase so does the bus traffic. This is why SMP systems usually only scale up to around 8 processors.

Having the processors all on the same bus starts to present physical problems as well. Physical properties of wires only allow them to be laid out at certain distances from each other and to only have certain lengths. With processors that run at many gigahertz the speed of light starts to become a real consideration in how long it takes messages to move around a system.

Note that system software usually has no part in this process, although programmers should be aware of what the hardware is doing underneath in response to the programs they design to maximise performance.
-->
### SMP系統中的緩存獨占性 Cache exclusivity in SMP systems

在“深度緩存”一節中，我們描述了包含v獨占緩存。一般來說，L1緩存通常是包含的——L1緩存中的所有數據也都駐留在L2緩存中。在多處理器系統中，一個包含L1緩存意味著只有L2緩存需要snoop內存流量來保持一致性，因為L2的任何變化都會被L1反映出來。這降低了L1的複雜性，並將其從窺探過程中分離出來，從而使其更快。


通常，大多數現代高端(例如，不針對嵌入式)處理器對L1緩存有寫通策略，對較低級別的緩存有寫回策略。這有幾個原因。由於在這類處理器中，L2緩存幾乎完全是在晶片上進行的，而且通常非常快，因此L1寫通的懲罰並不是主要考慮的問題。此外，由於L1大小較小，將來不太可能讀取的書面數據池可能會對有限的L1資源造成污染。另外，寫入到L1不需要擔心它是否有顯著的髒數據，因此可以將額外的相干邏輯傳遞給L2(正如我們所提到的，L2在緩存相乾性方面已經有了更大的作用)。
<!--
In the section called “Cache in depth” we described inclusive v exclusive caches. In general, L1 caches are usually inclusive — that is all data in the L1 cache also resides in the L2 cache. In a multiprocessor system, an inclusive L1 cache means that only the L2 cache need snoop memory traffic to maintain coherency, since any changes in L2 will be guaranteed to be reflected by L1. This reduces the complexity of the L1 and de-couples it from the snooping process allowing it to be faster.

Again, in general, most all modern high-end (e.g. not targeted at embedded) processors have a write-through policy for the L1 cache, and a write-back policy for the lower level caches. There are several reasons for this. Since in this class of processors L2 caches are almost exclusively on-chip and generally quite fast the penalties from having L1 write-through are not the major consideration. Further, since L1 sizes are small, pools of written data unlikely to be read in the future could cause pollution of the limited L1 resource. Additionally, a write-through L1 does not have to be concerned if it has outstanding dirty data, hence can pass the extra coherency logic to the L2 (which, as we mentioned, already has a larger part to play in cache coherency).
-->
### 超線程 Hyperthreading

現代處理器的大部分時間都花在等待內存層次結構中的慢得多的設備交付數據進行處理上。


因此，保持處理器資源充足的策略是至關重要的。一種策略是包含足夠的寄存器和狀態邏輯，以便兩個指令流可以同時處理。這使得一個CPU可以像兩個CPU一樣查找所有意圖和目的。


雖然每個CPU都有自己的寄存器，但是它們仍然必須共享核心邏輯、緩存以及從CPU到內存的輸入和輸出頻寬。因此，雖然兩個指令流可以保持處理器的核心邏輯更繁忙，但性能的提高不會像擁有兩個物理上獨立的cpu那樣顯著。通常性能改進低於20%  <!-- (XXX檢查) --> ，但是根據工作負載的不同，性能改進可能會大大提高或降低。
<!--
Much of the time of a modern processor is spent waiting for much slower devices in the memory hierarchy to deliver data for processing.

Thus strategies to keep the pipeline of the processor full are paramount. One strategy is to include enough registers and state logic such that two instruction streams can be processed at the same time. This makes one CPU look for all intents and purposes like two CPUs.

While each CPU has its own registers, they still have to share the core logic, cache and input and output bandwidth from the CPU to memory. So while two instruction streams can keep the core logic of the processor busier, the performance increase will not be as great has having two physically separate CPUs. Typically the performance improvement is below 20% (XXX check), however it can be drastically better or worse depending on the workloads.
-->
### 多核心 Multi Core

隨著在晶片上安裝越來越多的晶體管的能力的提高，將兩個或多個處理器放在同一物理封裝中成為可能。最常見的是雙核，即兩個處理器核在同一個晶片中。與超線程不同，這些內核是完整的處理器，因此看起來像兩個物理上獨立的處理器，就像SMP系統一樣。


雖然處理器通常有自己的L1緩存，但是它們必須共享連接到主內存和其他設備的匯流排。因此，性能不像完整的SMP系統那麼好，但是比超線程系統要好得多(實際上，每個內核仍然可以實現超線程以獲得額外的增強)。


多核處理器也有一些與性能無關的優點。如前所述，處理器之間的外部物理匯流排具有物理限制;通過在同一塊矽上的處理器彼此非常接近，這些問題可以解決。多核處理器的功耗要求遠遠低於兩個獨立處理器。這意味著需要散熱的熱量更少，這在數據中心應用程序中是一個很大的優勢，在數據中心中，計算機被打包在一起，冷卻的考慮可能是相當大的。通過將內核放在相同的物理包中，它使多處理器在其他應用程序(如筆記本電腦)中變得實用。而且只生產一個晶片比生產兩個晶片要便宜得多。

<!--
With increased ability to fit more and more transistors on a chip, it became possible to put two or more processors in the same physical package. Most common is dual-core, where two processor cores are in the same chip. These cores, unlike hyperthreading, are full processors and so appear as two physically separate processors a la a SMP system.

While generally the processors have their own L1 cache, they do have to share the bus connecting to main memory and other devices. Thus performance is not as great as a full SMP system, but considerably better than a hyperthreading system (in fact, each core can still implement hyperthreading for an additional enhancement).

Multi core processors also have some advantages not performance related. As we mentioned, external physical buses between processors have physical limits; by containing the processors on the same piece of silicon extremely close to each other some of these problems can be worked around. The power requirements for multi core processors are much less than for two separate processors. This means that there is less heat needing to be dissipated which can be a big advantage in data centre applications where computers are packed together and cooling considerations can be considerable. By having the cores in the same physical package it makes muti-processing practical in applications where it otherwise would not be, such as laptops. It is also considerably cheaper to only have to produce one chip rather than two.
-->
### 集群 Clusters

許多應用程序需要的系統數量遠遠大於SMP系統能夠擴展到的處理器數量。進一步擴展系統的一種方法是集群。


集群就是一些能夠相互通信的個人計算機。在硬件層面上，系統彼此不瞭解;把個人電腦拼接在一起的任務由軟件來完成。


像MPI這樣的軟件允許程序員編寫他們的軟件，然後將程序的一部分“外包”給系統中的其他計算機。例如，映像循環執行數千次執行獨立操作(循環的疊代不影響任何其他疊代)。在一個集群中有4台計算機，該軟件可以讓每台計算機執行250次循環。


計算機之間的互連各不相同，速度可能與互聯網連接一樣慢，也可能與專用專用匯流排(Infiniband)一樣快。但是，無論互連是什麼，它仍然會在內存層次結構中處於更低的位置，並且比RAM慢得多。因此，當每個CPU都需要訪問可能存儲在另一台計算機RAM中的數據時，集群將不能很好地運行;由於每次發生這種情況，軟件都需要從另一台計算機請求數據的副本，在處理器完成任何工作之前，通過慢速連結複製到本地RAM中。


然而，許多應用程序不需要在計算機之間不斷地複製。一個大規模的例子是SETI@Home，在那裡從無線電天線收集的數據被分析為外星生命的跡象。每台計算機可以分配幾分鐘的數據進行分析，並且只需要彙報它所發現的內容的摘要。SETI@Home實際上是一個非常大的專用集群。


另一個應用是圖象渲染，尤其是電影中的特效。每台電腦都可以獲得電影的一幀畫面，其中包含線框模型、紋理和光源，需要將它們組合(渲染)成我們現在使用的紋理特效。由於每一幀都是靜態的，一旦計算機有了初始輸入，它就不需要再進行通信，直到最後一幀準備好返回併合併到移動中。例如，《指環王》在運行Linux的大型集群上渲染了特效。
<!--
Many applications require systems much larger than the number of processors a SMP system can scale to. One way of scaling up the system further is a cluster.

A cluster is simply a number of individual computers which have some ability to talk to each other. At the hardware level the systems have no knowledge of each other; the task of stitching the individual computers together is left up to software.

Software such as MPI allow programmers to write their software and then "farm out" parts of the program to other computers in the system. For example, image a loop that executes several thousand times performing independent action (that is no iteration of the loop affects any other iteration). With four computers in a cluster, the software could make each computer do 250 loops each.

The interconnect between the computers varies, and may be as slow as an internet link or as fast as dedicated, special buses (Infiniband). Whatever the interconnect, however, it is still going to be further down the memory hierarchy and much, much slower than RAM. Thus a cluster will not perform well in a situation when each CPU requires access to data that may be stored in the RAM of another computer; since each time this happens the software will need to request a copy of the data from the other computer, copy across the slow link and into local RAM before the processor can get any work done.

However, many applications do not require this constant copying around between computers. One large scale example is SETI@Home, where data collected from a radio antenna is analysed for signs of Alien life. Each computer can be distributed a few minutes of data to analyse, and only needs report back a summary of what it found. SETI@Home is effectively a very large, dedicated cluster.

Another application is rendering of images, especially for special effects in films. Each computer can be handed a single frame of the movie which contains the wire-frame models, textures and light sources which needs to be combined (rendered) into the amazing special effects we now take for grained. Since each frame is static, once the computer has the initial input it does not need any more communication until the final frame is ready to be sent back and combined into the move. For example the block-buster Lord of the Rings had their special effects rendered on a huge cluster running Linux.
-->
### 非統一內存訪問 Non-Uniform Memory Access

非統一內存訪問(通常縮寫為NUMA)幾乎與上面提到的集群系統相反。在集群系統中，它是由連接在一起的各個節點組成的，但是節點之間的連接是高度專門化的(而且昂貴!)相對於集群系統，硬件不知道節點之間的連接，在NUMA系統中，軟件不知道系統的佈局，硬件做所有的工作來連接節點。


“非均勻內存訪問”一詞來源於這樣一個事實，即RAM可能不是CPU的本地存儲器，因此數據可能需要從一定距離之外的節點訪問。這顯然需要更長的時間，而且與直接連接RAM的單個處理器或SMP系統相比，它總是需要恆定的(統一的)訪問時間。
<!--
Non-Uniform Memory Access, more commonly abbreviated to NUMA, is almost the opposite of a cluster system mentioned above. As in a cluster system it is made up of individual nodes linked together, however the linkage between nodes is highly specialised (and expensive!). As opposed to a cluster system where the hardware has no knowledge of the linkage between nodes, in a NUMA system the software has no (well, less) knowledge about the layout of the system and the hardware does all the work to link the nodes together.

The term non uniform memory access comes from the fact that RAM may not be local to the CPU and so data may need to be accessed from a node some distance away. This obviously takes longer, and is in contrast to a single processor or SMP system where RAM is directly attached and always takes a constant (uniform) time to access.
-->
### NUMA機器佈局 (NUMA Machine Layout)

在一個系統中有如此多的節點在相互通信，最小化每個節點之間的距離至關重要。顯然，最好是每個節點都有一個到其他節點的直接連結，因為這樣可以最小化任何節點查找數據所需的距離。當節點的數量開始增長到成百上千的時候，這不是一個實際的情況，就像大型超級計算機那樣;如果你還記得你高中時的數學，這個問題基本上是一次兩個節點(每個節點與另一個節點通信)的組合，並且會增長n!/2*(n-2)!。


為了克服這種指數增長的問題，可以使用替代佈局來權衡節點之間的距離和所需的互連。現代NUMA體系結構中常見的一種佈局是hypercube。


超立方體有嚴格的數學定義(遠遠超出了這個討論)，但是由於立方體是正方形的三維對應體，所以超立方體是立方體的四維對應體。
<!--
With so many nodes talking to each other in a system, minimising the distance between each node is of paramount importance. Obviously it is best if every single node has a direct link to every other node as this minimises the distance any one node needs to go to find data. This is not a practical situation when the number of nodes starts growing into the hundreds and thousands as it does with large supercomputers; if you remember your high school maths the problem is basically a combination taken two at a time (each node talking to another), and will grow n!/2*(n-2)!.

To combat this exponential growth alternative layouts are used to trade off the distance between nodes with the interconnects required. One such layout common in modern NUMA architectures is the hypercube.

A hypercube has a strict mathematical definition (way beyond this discussion) but as a cube is a 3 dimensional counterpart of a square, so a hypercube is a 4 dimensional counterpart of a cube.
-->
> 圖3.8。一個超立方體 (A Hypercube)

![](http://www.bottomupcs.com/chapter02/figures/hypercube.png)

> 一個超立方體的例子。超多維數據集提供了節點間距離和所需互連數量之間的良好平衡。

<!--
> An example of a hypercube. Hypercubes provide a good trade off between distance between nodes and number of interconnections required.
-->

上面我們可以看到外部立方體包含四個8個節點。任何節點與另一個節點通信所需的最大路徑數為3。當另一個多維數據集被放在這個多維數據集中，我們現在有兩倍的處理器數量，但是最大路徑成本只增加到4。這意味著隨著處理器數量增加2n，最大路徑成本只線性增長。
<!--
Above we can see the outer cube contains four 8 nodes. The maximum number of paths required for any node to talk to another node is 3. When another cube is placed inside this cube, we now have double the number of processors but the maximum path cost has only increased to 4. This means as the number of processors grow by 2n the maximum path cost grows only linearly.
-->
### 緩存一致性 Cache Coherency

緩存相乾性仍然可以在NUMA系統中保持(這被稱為Cache - coherence NUMA系統，或ccNUMA)。如前所述，用於在SMP系統中保持處理器緩存一致性的基於廣播的方案在大型NUMA系統中不會擴展到數百甚至數千個處理器。NUMA系統中緩存一致性的一種常見方案稱為基於目錄的模型。在此模型中，處理器在系統中與專用緩存目錄硬件通信。目錄硬件維護每個處理器的一致圖象;這個抽象對處理器隱藏了NUMA系統的工作。


基於Censier和Feautrier目錄的方案維護一個中心目錄，其中每個內存塊都有一個標記位，即每個處理器的有效位和一個髒位。當處理器將內存讀入其緩存時，該目錄設置該處理器的有效位。


當處理器希望寫入高速緩存線時，目錄需要為內存塊設置髒位。這涉及到向那些正在使用高速緩存線路的處理器(並且僅向那些設置了標誌的處理器發送無效消息;避免廣播流量)。


這之後其他處理器應該讀取內存塊的目錄會發現髒點集。將需要更新的目錄從處理器高速緩存線路目前設置了有效位,髒數據寫回主存,然後將該數據提供給請求處理器設置為請求處理器有效位。注意，這對請求處理程序是透明的，目錄可能需要從非常接近或非常遠的地方獲取數據。


顯然，讓數千個處理器通信到一個目錄也不能很好地擴展。該方案的擴展包括使用一個單獨的協議在彼此之間通信的目錄層次結構。這些目錄可以使用更通用的通信網絡在彼此之間進行通信，而不是使用CPU匯流排，從而可以擴展到更大的系統。
<!--
Cache coherency can still be maintained in a NUMA system (this is referred to as a cache-coherent NUMA system, or ccNUMA). As we mentioned, the broadcast based scheme used to keep the processor caches coherent in an SMP system does not scale to hundreds or even thousands of processors in a large NUMA system. One common scheme for cache coherency in a NUMA system is referred to as a directory based model. In this model processors in the system communicate to special cache directory hardware. The directory hardware maintains a consistent picture to each processor; this abstraction hides the working of the NUMA system from the processor.

The Censier and Feautrier directory based scheme maintains a central directory where each memory block has a flag bit known as the valid bit for each processor and a single dirty bit. When a processor reads the memory into its cache, the directory sets the valid bit for that processor.

When a processor wishes to write to the cache line the directory needs to set the dirty bit for the memory block. This involves sending an invalidate message to those processors who are using the cache line (and only those processors whose flag are set; avoiding broadcast traffic).

After this should any other processor try to read the memory block the directory will find the dirty bit set. The directory will need to get the updated cache line from the processor with the valid bit currently set, write the dirty data back to main memory and then provide that data back to the requesting processor, setting the valid bit for the requesting processor in the process. Note that this is transparent to the requesting processor and the directory may need to get that data from somewhere very close or somewhere very far away.

Obviously having thousands of processors communicating to a single directory does also not scale well. Extensions to the scheme involve having a hierarchy of directories that communicate between each other using a separate protocol. The directories can use a more general purpose communications network to talk between each other, rather than a CPU bus, allowing scaling to much larger systems.
-->
### NUMA應用程序 (NUMA Applications)

NUMA系統最適合於需要處理器和內存之間進行大量交互的問題類型。例如，在天氣模擬中，一個常見的習語是將環境劃分成小的“盒子”，它們以不同的方式響應(例如，海洋和陸地反射或儲存不同數量的熱量)。當模擬運行時，會有小的變化來觀察整體的結果。由於每個盒子都會影響周圍的盒子(例如，多一點太陽意味著一個盒子會放出更多的熱量，影響到它旁邊的盒子)，就會有很多的交流(與渲染過程中的單個圖象幀相比，每一個都不會影響到另一個)。如果你在模擬一場車禍，模擬車的每一個小盒子都會以某種方式摺疊起來並吸收一定的能量，類似的過程可能會發生。


儘管軟件並不直接知道底層系統是NUMA系統，但程序員在為系統編程時需要小心，以獲得最大的性能。顯然，將內存放在將要使用它的處理器附近將獲得最佳性能。程序員需要使用分析等技術來分析改採用的代碼路徑，以及他們的代碼對系統提取最佳性能的影響。
<!--
NUMA systems are best suited to the types of problems that require much interaction between processor and memory. For example, in weather simulations a common idiom is to divide the environment up into small "boxes" which respond in different ways (oceans and land reflect or store different amounts of heat, for example). As simulations are run, small variations will be fed in to see what the overall result is. As each box influences the surrounding boxes (e.g. a bit more sun means a particular box puts out more heat, affecting the boxes next to it) there will be much communication (contrast that with the individual image frames for a rendering process, each of which does not influence the other). A similar process might happen if you were modelling a car crash, where each small box of the simulated car folds in some way and absorbs some amount of energy.

Although the software has no directly knowledge that the underlying system is a NUMA system, programmers need to be careful when programming for the system to get maximum performance. Obviously keeping memory close to the processor that is going to use it will result in the best performance. Programmers need to use techniques such as profiling to analyse the code paths taken and what consequences their code is causing for the system to extract best performance.
-->
### 內存排序、鎖定和原子操作 (Memory ordering, locking and atomic operations)

多層緩存、超標量多處理器體系結構帶來了一些與程序員如何看待處理器運行代碼有關的有趣問題。


假設程序代碼同時在兩個處理器上運行，兩個處理器有效地共享一個大的內存區域。如果一個處理器發出存儲指令，將寄存器值放入內存中，它什麼時候能確定另一個處理器加載了該內存，它將看到正確的值?


在最簡單的情況下，系統可以保證如果程序執行存儲指令，任何後續的加載指令都會看到這個值。這被稱為嚴格的內存排序，因為規則不允許移動。您應該開始意識到為什麼這類事情會嚴重妨礙系統的性能。


很多時候，並不要求內存排序這麼嚴格。程序員可以確定需要確保所有出色的操作都是全局可見的點，但是在這些點之間可能有很多指令，語義並不重要。


例如，下面的情況。
<!--
The multi-level cache, superscalar multi-processor architecture brings with it some interesting issues relating to how a programmer sees the processor running code.

Imagine program code is running on two processors simultaneously, both processors sharing effectively one large area of memory. If one processor issues a store instruction, to put a register value into memory, when can it be sure that the other processor does a load of that memory it will see the correct value?

In the simplest situation the system could guarantee that if a program executes a store instruction, any subsequent load instructions will see this value. This is referred to as strict memory ordering, since the rules allow no room for movement. You should be starting to realise why this sort of thing is a serious impediment to performance of the system.

Much of the time, the memory ordering is not required to be so strict. The programmer can identify points where they need to be sure that all outstanding operations are seen globally, but in between these points there may be many instructions where the semantics are not important.

Take, for example, the following situation.
-->
> 3.1 的例子。內存排序 (Memory Ordering)

```c
typedef struct {
  int a;
  int b;
} a_struct;

/*
 * Pass in a pointer to be allocated as a new structure
 */
void get_struct(a_struct *new_struct)
{
	void *p = malloc(sizeof(a_struct));

	/* We don't particularly care what order the following two
	 * instructions end up acutally executing in */
	p->a = 100;
	p->b = 150;

	/* However, they must be done before this instruction.
	 * Otherwise, another processor who looks at the value of p
	 * could find it pointing into a structure whose values have
	 * not been filled out.
	 */
	new_struct = p;
}
```

在本例中，我們有兩個存儲，可以按照任何特定順序執行，因為它適合處理器。但是，在最後一種情況下，指針只能在已知完成前兩個存儲之後更新。否則，另一個處理器可能會查看p的值，跟隨指針到內存，加載它，並得到一些完全不正確的值!


為了表明這一點，加載和存儲必須具有描述它們必須具有的行為的語義。內存語義是用籬笆來描述的，籬笆指示如何在負載或存儲周圍重新排序負載和存儲。


默認情況下，加載或存儲可以在任何地方重新排序。


獲取語義就像一個籬笆，只允許裝載和存儲通過它向下移動。也就是說，當這個加載或存儲完成時，您可以保證以後的任何加載或存儲都將看到這個值(因為它們不能移動到它上面)。


發佈語義與此相反，它是一個籬笆，允許任何加載或存儲在它(向上移動)之前完成，但在它向下移動之前什麼也不做。因此，在處理帶有發佈語義的加載或存儲時，您可以存儲任何早期加載或存儲已經完成。
<!--
In this example, we have two stores that can be done in any particular order, as it suits the processor. However, in the final case, the pointer must only be updated once the two previous stores are known to have been done. Otherwise another processor might look at the value of p, follow the pointer to the memory, load it, and get some completely incorrect value!

To indicate this, loads and stores have to have semantics that describe what behaviour they must have. Memory semantics are described in terms of fences that dictate how loads and stores may be reordered around the load or store.

By default, a load or store can be re-ordered anywhere.

Acquire semantics is like a fence that only allows load and stores to move downwards through it. That is, when this load or store is complete you can be guaranteed that any later load or stores will see the value (since they can not be moved above it).

Release semantics is the opposite, that is a fence that allows any load or stores to be done before it (move upwards), but nothing before it to move downwards past it. Thus, when load or store with release semantics is processed, you can be store that any earlier load or stores will have been complete.
-->
> 圖3.9。獲取和發佈語義 (Acquire and Release semantics)

![](http://www.bottomupcs.com/chapter02/figures/memorder.png)

> 是關於具有獲取和發佈語義的操作的有效重組的說明。

> An illustration of valid reorderings around operations with acquire and release semantics.

一個完整的內存籬笆是兩者的結合;任何負載或存儲可以在當前負載或存儲周圍的任何方向重新排序。


最嚴格的內存模型將為每個操作使用全內存圍欄。最弱的模型會將每個負載和存儲作為正常的可重新排序指令。
<!--
A full memory fence is a combination of both; where no loads or stores can be reordered in any direction around the current load or store.

The strictest memory model would use a full memory fence for every operation. The weakest model would leave every load and store as a normal re-orderable instruction.
-->
### 處理器和內存模型 Processors and memory models

不同的處理器實現不同的內存模型。


x86(和AMD64)處理器有一個相當嚴格的內存模型;所有儲存都有發佈語義(也就是說，儲存的結果保證以後的加載或存儲都能看到)，但是所有的加載都有正常的語義。鎖首碼提供內存圍欄。


除非明確告知，安騰允許所有負載和存儲都是正常的。  <!-- XXX -->
<!--
Different processors implement different memory models.

The x86 (and AMD64) processor has a quite strict memory model; all stores have release semantics (that is, the result of a store is guaranteed to be seen by any later load or store) but all loads have normal semantics. lock prefix gives memory fence.

Itanium allows all load and stores to be normal, unless explicitly told. XXX
-->
### 鎖定 Locking

瞭解每個體系結構的內存排序需求對所有程序員都不實用，而且會使程序難以跨不同處理器類型進行移植和調試。


程序員使用更高級別的抽象稱為鎖定，以允許在有多個cpu時同時操作程序。


當一個程序獲得一段代碼上的鎖時，沒有其他處理器能夠獲得鎖，直到它被釋放。在任何關鍵代碼片段之前，處理器必須嘗試獲取鎖;如果它不能擁有它，它就不會繼續。


您可以看到這是如何與前一節中內存排序語義的命名聯繫在一起的。我們希望確保在獲取鎖之前，沒有任何應該由鎖保護的操作在它之前被重新排序。這就是獲取語義的工作方式。


相反，在釋放鎖時，我們必須確保在持有鎖時所做的每個操作都是完整的(還記得之前更新指針的例子嗎?)這是發佈語義。


有許多可用的軟件庫允許程序員不必擔心內存語義的細節，只需使用更高級別的lock()和unlock()抽象即可。
<!--
Knowing the memory ordering requirements of each architecture is not practical for all programmers, and would make programs difficult to port and debug across different processor types.

Programmers use a higher level of abstraction called locking to allow simultaneous operation of programs when there are multiple CPUs.

When a program acquires a lock over a piece of code, no other processor can obtain the lock until it is released. Before any critical pieces of code, the processor must attempt to take the lock; if it can not have it, it does not continue.

You can see how this is tied into the naming of the memory ordering semantics in the previous section. We want to ensure that before we acquire a lock, no operations that should be protected by the lock are re-ordered before it. This is how acquire semantics works.

Conversely, when we release the lock, we must be sure that every operation we have done whilst we held the lock is complete (remember the example of updating the pointer previously?). This is release semantics.

There are many software libraries available that allow programmers to not have to worry about the details of memory semantics and simply use the higher level of abstraction of lock() and unlock().
-->
### 鎖定的困難 Locking difficulties

鎖定方案使編程更加複雜，因為死鎖程序是可能的。想像一下，如果一個處理器當前持有一些數據的鎖，並且正在等待另一些數據的鎖。如果另一個處理器正在等待第一個處理器持有的鎖，然後才解鎖第二個鎖，那麼就會出現死鎖情況。每個處理器都在等待另一個處理器，沒有其他的鎖，它們都不能繼續。


這種情況通常是由於一種微妙的種族狀況引起的;最難追蹤的bug之一。如果兩個處理器依賴於在特定時間順序中發生的操作，那麼總是有可能發生竟態條件。來自另一個星系的爆炸恆星的伽馬射線可能會擊中其中一個處理器，使其跳過一個節拍，打亂操作順序。通常會出現如上所述的死鎖情況。正是由於這個原因，程序排序需要由語義來保證，而不是依賴於一次特定的行為。(我不知道該怎麼說才好)。


類似的情況是死鎖的反面，稱為活鎖。避免死鎖的一種策略可能是使用“禮貌”鎖;這是你對任何提出要求的人都要放棄的。這個禮貌可能導致兩個線程不斷給彼此鎖,沒有永遠的鎖足夠長的時間來完成關鍵工作和完成鎖(類似的情況在現實生活中可能是兩人在一扇門的同時,都說“不,你第一次,我堅持”。都沒有通過門!)
<!--
Locking schemes make programming more complicated, as it is possible to deadlock programs. Imagine if one processor is currently holding a lock over some data, and is currently waiting for a lock for some other piece of data. If that other processor is waiting for the lock the first processor holds before unlocking the second lock, we have a deadlock situation. Each processor is waiting for the other and neither can continue without the others lock.

Often this situation arises because of a subtle race condition; one of the hardest bugs to track down. If two processors are relying on operations happening in a specific order in time, there is always the possibility of a race condition occurring. A gamma ray from an exploding star in a different galaxy might hit one of the processors, making it skip a beat, throwing the ordering of operations out. What will often happen is a deadlock situation like above. It is for this reason that program ordering needs to be ensured by semantics, and not by relying on one time specific behaviours. (XXX not sure how i can better word that).

A similar situation is the opposite of deadlock, called livelock. One strategy to avoid deadlock might be to have a "polite" lock; one that you give up to anyone who asks. This politeness might cause two threads to be constantly giving each other the lock, without either ever taking the lock long enough to get the critical work done and be finished with the lock (a similar situation in real life might be two people who meet at a door at the same time, both saying "no, you first, I insist". Neither ends up going through the door!).
-->
### 鎖定策略 Locking strategies

在底層，有許多實現鎖行為的不同策略。


一個簡單的鎖只有兩種狀態——鎖定或解鎖，稱為互斥(互斥的縮寫;也就是說，如果一個人擁有它，另一個人就不能擁有)。


然而，有許多方法可以實現互斥鎖。在最簡單的情況下，我們有通常稱為自旋鎖的東西。對於這種類型的鎖，處理器處於一個緊的循環中，等待獲取鎖;相當於經常說“我現在能擁有它嗎”，就像一個小孩可能會問父母一樣。


這個策略的問題在於它實際上是在浪費時間。當處理器一直坐在那裡請求鎖時，它並沒有做任何有用的工作。對於可能只持有鎖很短時間的鎖，這可能是合適的，但在許多情況下，持有鎖的時間可能要長得多。


因此，另一個策略是睡在鎖上。在這種情況下，如果處理器沒有鎖，它將開始做一些其他的工作，等待鎖可用的通知(我們將在後面的章節中看到操作系統如何切換進程並給處理器更多的工作要做)。


然而，互斥鎖只是信號量的一個特例，這個信號量是由荷蘭計算機科學家Dijkstra發明的。在有多個可用資源的情況下，可以設置信號量來統計對資源的訪問。在資源數量為1的情況下，有一個互斥量。信號量的操作可以在任何算法書中詳細描述。


然而，這些鎖定方案仍然存在一些問題。在許多情況下，大多數人只想讀取很少更新的數據。所有希望讀取數據的處理器都需要使用鎖，這會導致鎖爭用，因為每個人都在等待為某些數據獲得相同的鎖，所以完成的工作更少。
<!--
Underneath, there are many different strategies for implementing the behaviour of locks.

A simple lock that simply has two states - locked or unlocked, is referred to as a mutex (short for mutual exclusion; that is if one person has it the other can not have it).

There are, however, a number of ways to implement a mutex lock. In the simplest case, we have what its commonly called a spinlock. With this type of lock, the processor sits in a tight loop waiting to take the lock; equivalent to it saying "can I have it now" constantly much as a young child might ask of a parent.

The problem with this strategy is that it essentially wastes time. Whilst the processor is sitting constantly asking for the lock, it is not doing any useful work. For locks that are likely to be only held locked for a very short amount of time this may be appropriate, but in many cases the amount of time the lock is held might be considerably longer.

Thus another strategy is to sleep on a lock. In this case, if the processor can not have the lock it will start doing some other work, waiting for notification that the lock is available for use (we see in future chapters how the operating system can switch processes and give the processor more work to do).

A mutex is however just a special case of a semaphore, famously invented by the Dutch computer scientist Dijkstra. In a case where there are multiple resources available, a semaphore can be set to count accesses to the resources. In the case where the number of resources is one, you have a mutex. The operation of semaphores can be detailed in any algorithms book.

These locking schemes still have some problems however. In many cases, most people only want to read data which is updated only rarely. Having all the processors wanting to only read data require taking a lock can lead to lock contention where less work gets done because everyone is waiting to obtain the same lock for some data.

### 原子操作 Atomic Operations
-->